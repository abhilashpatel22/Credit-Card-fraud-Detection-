{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "8046d3ba-d2da-6eae-1ecb-9509216f78a2",
    "_uuid": "9b09dde4328a30a3b322fdaf655e7ac5d3797952"
   },
   "source": [
    "Hello my Friends. After almost 2 years of experiance data science, I am finally making my first post!\n",
    "The credit fraud dataset is a really nice exercise to start my journey in Kaggle. It's easy and clean, no much effort needed since all components are already PCA-ed. \n",
    "\n",
    "For this exercise I did basic analysis of the dataset and fitted the values to a few training models with minimal preprocessing, little cross-validation trickery and no fancy boosting. The results were too good to be true, probably thanks to the pre-done PCA. I intend to present my work as a showcase of my beginner's skill and would appreciate any kind of feedback on how I can improve.\n",
    "\n",
    "Anyway, let's start."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d60418ba-1caa-89fc-dca6-57640abdc838",
    "_uuid": "516f8f8808471d1f4440708145e46510c5aafb60"
   },
   "source": [
    "We first import the required modules, you can already see where I am going."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d0bfdcde-6a62-85d9-010d-c917e1ec523c",
    "_uuid": "3e03c376826513dba66b4e488ed4b277c7cba0ce",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing, cross_validation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn import linear_model\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d4bd5727-06ff-11ef-7288-2947ac0c72a2",
    "_uuid": "18933fcf3ab83b5496f56a3915fa85dfa75b7c5e"
   },
   "source": [
    "Oops I seem to be using old modules, I don't where else to find cross-validation though.\n",
    "\n",
    "Let's read the file and see what's in it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "09467dec-f796-0898-2f0f-790b4423760a",
    "_uuid": "6060b5859ec8146bcd3bb5cd6eae4271b7fbd3c3",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../input/creditcard.csv')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "426b18d8-182e-1608-9e0d-93edeb853555",
    "_uuid": "f732098438ccf7a3aa6bed6a4dddd893f1875964"
   },
   "source": [
    "Scrolling to the right we can see the columns don't really mean much as a result of PCA. Class is either 0 or 1. Time is indexed from zero which doesn't show much other than it's sequential structure.\n",
    "\n",
    "Let's go straight into exploratory analysis. We will start with inspection of whether the columns are really PCA-ed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "35f6d27e-c67a-af9d-7a8f-76c688d96442",
    "_uuid": "f3d03f994d2af2eb2e0b75918a78d1e37144c52d",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_corr(df):\n",
    "    #### Checking correlation of PCA variables\n",
    "    corr = df.corr()\n",
    "    mask = np.zeros_like(corr, dtype=np.bool)\n",
    "    mask[np.triu_indices_from(mask)] = True\n",
    "    \n",
    "    fig1, ax1 = plt.subplots()\n",
    "    corrplot = sns.heatmap(corr, mask=mask, square=True, ax=ax1)\n",
    "    plt.yticks(rotation=0)\n",
    "    plt.xticks(rotation=90)\n",
    "\n",
    "plot_corr(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b01afda3-d560-9ff4-8891-d836987508e8",
    "_uuid": "1c1472a7e9f28a9b76eadf9405047ef5ccf01286"
   },
   "source": [
    "We see that the PCA columns are not correlated to each other. Power of PCA.\n",
    "Time, amount and class quite randomly correlated to the others, doesn't look suspicious here so I will leave them as they are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "edbb337f-eef8-d3e1-1b3f-dbf0d81fa77e",
    "_uuid": "51cbe1ef2255605e63b81b939e591d9619e9b83c"
   },
   "source": [
    "Let's look at the fraud incidents over years now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "3afda42d-bae8-890a-834b-0399699286b9",
    "_uuid": "3bdd429a3477ef4ecf43063e18c340046966ad97",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_time(df):\n",
    "##### Checking the time series (fraud)\n",
    "    df1 = df.loc[df['Class']==1, :]\n",
    "    df2 = df.loc[df['Class']==0, :]\n",
    "    \n",
    "    fig2, ax2 = plt.subplots()\n",
    "    x1 = df1['Time']\n",
    "    y1 = df1['Amount']\n",
    "    x2 = df2['Time']\n",
    "    y2 = df2['Amount']\n",
    "    ax2.scatter(x2, y2, alpha=0.2, color = 'blue')\n",
    "    ax2.scatter(x1, y1, alpha=0.8, color='red')\n",
    "    ax2.set_xlim((0, max(x2)))\n",
    "    ax2.set_ylim((0, max(y2)))\n",
    "    \n",
    "plot_time(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "0c36faab-b9e3-3ec7-fce2-234d2c67bd0d",
    "_uuid": "4e5365f7daa271a9e6dbde4ec74b6820d3444e21"
   },
   "source": [
    "Pretty random to me. Could have done a bit more to sum them counts or amounts up by year but I didn't do it.\n",
    "\n",
    "I wonder if the one at the top right corner corresponds to ...Lehman Bros? ...Madoff?  \n",
    "\n",
    "Let's look at the distribution next."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "5508ec08-2819-b733-f6a2-167d594c9163",
    "_uuid": "f05ba8286411fa456cc041c409869c068deba988",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def plot_dist(df):\n",
    "    ##### Boxplot Violin plot to show distribution\n",
    "    fig3, (ax3_1, ax3_2) = plt.subplots(ncols=2, sharey=True)\n",
    "    sns.violinplot(x=df['Class'], y=df['Amount'], ax=ax3_1)\n",
    "    sns.boxplot(x=df['Class'], y=df['Amount'], ax=ax3_2)\n",
    "    \n",
    "plot_dist(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "362932e4-634a-afbc-b336-40f1b5455914",
    "_uuid": "37e809b09d7395fbee16a7cd8a92deba6c14c594"
   },
   "source": [
    "We can see the the outliers heavily skew the plot with class 0. Not much to see here.\n",
    "\n",
    "Now that we have done our due-diligence in charting, let's build some models and compare their accuracies. Here we are going to use KMeans, Regression and RandomForest. Three very different kinds of algos.\n",
    "\n",
    "Let's show all the entire script and call them separately later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "562b0266-3b2b-51da-923b-8b547427e1b4",
    "_uuid": "2f5204f17190d5fd4c3a286cd129f496fdf2fd63",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preproc(df):\n",
    "    ##### Preprocessing\n",
    "    X = np.array(df.drop(['Time', 'Class'], 1).astype(float))\n",
    "    Y = np.array(df['Class'])\n",
    "    \n",
    "    X = preprocessing.scale(X)\n",
    "    \n",
    "    X_train, X_test, Y_train, Y_test = cross_validation.train_test_split(X, Y, test_size=0.3)\n",
    "\n",
    "    return(X_train, X_test, Y_train, Y_test)\n",
    "\n",
    "def mod_predict(x, y, model):\n",
    "    prediction = model.predict(x)\n",
    "    acc = abs(sum(prediction==y)/len(y))\n",
    "    if acc <= 0.5:\n",
    "        acc = 1 - acc\n",
    "    return(acc)\n",
    "\n",
    "def KMs(df):\n",
    "    X_train, X_test, Y_train, Y_test = preproc(df)\n",
    "    \n",
    "    clf = KMeans(n_clusters=2)\n",
    "    clf.fit(X_train)\n",
    "    \n",
    "    in_sample_acc = mod_predict(X_train, Y_train, clf)\n",
    "    out_sample_acc = mod_predict(X_test, Y_test, clf)\n",
    "    \n",
    "    print('In-Sample accuracy : ' + str(round(in_sample_acc, 4)) +'\\n' + 'Out-Sample accuracy : ' + str(round(out_sample_acc, 4)))\n",
    "\n",
    "def LogReg(df):\n",
    "    X_train, X_test, Y_train, Y_test = preproc(df)\n",
    "    \n",
    "    glm = linear_model.LogisticRegression()\n",
    "    glm.fit(X_train, Y_train)\n",
    "    \n",
    "    in_sample_acc = mod_predict(X_train, Y_train, glm)\n",
    "    out_sample_acc = mod_predict(X_test, Y_test, glm)\n",
    "    \n",
    "    print('In-Sample accuracy : ' + str(round(in_sample_acc, 4)) +'\\n' + 'Out-Sample accuracy : ' + str(round(out_sample_acc, 4)))\n",
    "    \n",
    "def randomforest(df):\n",
    "    X_train, X_test, Y_train, Y_test = preproc(df)\n",
    "    \n",
    "    rf = RandomForestClassifier()\n",
    "    rf.fit(X_train, Y_train)\n",
    "    \n",
    "    in_sample_acc = mod_predict(X_train, Y_train, rf)\n",
    "    out_sample_acc = mod_predict(X_test, Y_test, rf)\n",
    "    \n",
    "    print('In-Sample accuracy : ' + str(round(in_sample_acc, 4)) +'\\n' + 'Out-Sample accuracy : ' + str(round(out_sample_acc, 4)))\n",
    "    \n",
    "    imp = rf.feature_importances_\n",
    "    fig4, ax4 = plt.subplots()\n",
    "    sns.barplot(list(range(len(imp))), imp)\n",
    "    fig4.suptitle('PCA feature importance')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "e9d65cac-ba99-d1ec-f602-87de9f167966",
    "_uuid": "b88a86b42a08bd0f0fe6a012118fb5d7a20c40c0"
   },
   "source": [
    "Here, *preproc* does the preprocessing i.e. the centering, scaling and train/test splitting. *mod predict* takes one of the models and run it with either the train or test set and spits out an accuracy.\n",
    "\n",
    "Now let's run our models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "d59b6fcb-8f6a-28fb-20a7-98f6e2479ace",
    "_uuid": "c7ca7b6ab3008b1eb9466c47a2b2258420ff524a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('--------KMeans--------')\n",
    "KMs(df)\n",
    "print('--------Logistic Regression--------')\n",
    "LogReg(df)\n",
    "print('--------Random Forest--------')\n",
    "randomforest(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f148bf97-aefa-b607-8eb9-6dd3805ae5f9",
    "_uuid": "49c3a5b9f50b09badf957f371990cc72e61beed0"
   },
   "source": [
    "Just like we would have expected, Random Forest came up first, followed by Logistic Regression and KMeans. This is in the same order as the model complexities, namely the ensemble boosting of Random Forest, log likelihood maximisation of Logistic Regression and the whatever is in KMeans.\n",
    "\n",
    "Still, 99.99% accuracy is quite outrageous and I would like you to tell me if I am correct!\n",
    "\n",
    "Also I have included the relative importance of the PCA columns. Nothing interesting here other than that they are quite well dispersed, possible effect of PCA."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "957f5f6c-9961-2fe9-2363-058ea85de649",
    "_uuid": "6ee0c511ff6378221e3fd0c010b83ad199186d99"
   },
   "source": [
    "And there you go, thank you! Feedback appreciated!"
   ]
  }
 ],
 "metadata": {
  "_change_revision": 0,
  "_is_fork": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
